{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Music Clustering - Exploratory Analysis\n",
    "\n",
    "This notebook provides an interactive exploration of:\n",
    "1. Dataset statistics\n",
    "2. Model training\n",
    "3. Latent space visualization\n",
    "4. Clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add project root to path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src import (\n",
    "    VAE, CVAE, SimpleAutoencoder,\n",
    "    SongDataset, SongDatasetWithLabels,\n",
    "    extract_latent_features, apply_kmeans, reduce_tsne,\n",
    "    compute_all_metrics, compare_methods\n",
    ")\n",
    "from config import *\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset index\n",
    "df = pd.read_csv(DATASET_INDEX)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df['language'].value_counts())\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['language'].value_counts().plot(kind='bar')\n",
    "plt.title('Language Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "vae_model = VAE(input_dim=INPUT_DIM, hidden_dim=LATENT_DIM).to(device)\n",
    "vae_model.load_state_dict(torch.load('../vae_model.pth', map_location=device))\n",
    "vae_model.eval()\n",
    "\n",
    "print(\"VAE model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Latent Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = SongDataset(DATA_DIR, seq_len=SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Extract latent features\n",
    "latents, labels = extract_latent_features(vae_model, dataloader, device)\n",
    "print(f\"Latent features shape: {latents.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means\n",
    "cluster_labels = apply_kmeans(latents, n_clusters=K_CLUSTERS, random_state=RANDOM_SEED)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_all_metrics(labels, cluster_labels, latents)\n",
    "\n",
    "print(\"\\nClustering Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:.<30} {value:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using t-SNE\n",
    "embedding = reduce_tsne(latents, n_components=2, random_state=RANDOM_SEED)\n",
    "\n",
    "# Plot ground truth vs clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='coolwarm', s=5, alpha=0.7)\n",
    "axes[0].set_title('Ground Truth (Blue=English, Red=Bangla)', fontsize=14)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1')\n",
    "axes[0].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Predicted clusters\n",
    "axes[1].scatter(embedding[:, 0], embedding[:, 1], c=cluster_labels, cmap='viridis', s=5, alpha=0.7)\n",
    "axes[1].set_title('K-Means Clusters', fontsize=14)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/latent_visualization/notebook_tsne.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Autoencoder\n",
    "ae_model = SimpleAutoencoder(input_dim=INPUT_DIM, latent_dim=LATENT_DIM).to(device)\n",
    "ae_model.load_state_dict(torch.load('../autoencoder_model.pth', map_location=device))\n",
    "ae_model.eval()\n",
    "\n",
    "# Extract features\n",
    "ae_latents, _ = extract_latent_features(ae_model, dataloader, device)\n",
    "ae_clusters = apply_kmeans(ae_latents, n_clusters=K_CLUSTERS, random_state=RANDOM_SEED)\n",
    "ae_metrics = compute_all_metrics(labels, ae_clusters, ae_latents)\n",
    "ae_metrics['Method'] = 'Autoencoder'\n",
    "\n",
    "# Compare\n",
    "metrics['Method'] = 'VAE'\n",
    "compare_methods([metrics, ae_metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Investigate Sample MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample MFCC\n",
    "sample_idx = 0\n",
    "mfcc, label = dataset[sample_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(mfcc.numpy(), aspect='auto', cmap='viridis', origin='lower')\n",
    "plt.colorbar(label='MFCC Value')\n",
    "plt.title(f'Sample MFCC (Language: {\"English\" if label == 0 else \"Bangla\"})')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('MFCC Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Dataset exploration and balance verification\n",
    "- ✅ Loading pretrained VAE models\n",
    "- ✅ Extracting latent features\n",
    "- ✅ Clustering evaluation\n",
    "- ✅ t-SNE visualization\n",
    "- ✅ Model comparison\n",
    "\n",
    "**Key Finding:** Language clustering from audio alone is challenging, with modest performance metrics indicating the need for improved features or larger datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
